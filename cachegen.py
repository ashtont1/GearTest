# -*- coding: utf-8 -*-
"""cachegen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nk5C3vH8xbfbTGvHIw83v20dUsnG6Kc-
"""

import torch
import os
import numpy as np
import json
from transformers import AutoTokenizer, AutoModelForCausalLM

def dequant(xq, max1, dim=-1, quant_type="vector"):
    C = int(os.environ["BINS"]) // 2 - 1
    stride = 4
    MAX = int(os.environ["BINS"]) // 2 - 1
    C = MAX
    chunks = torch.chunk(xq, xq.shape[0] // stride)
    all_tokens = []

    for i in range(len(chunks)):
        length = len(chunks[i])
        chunk = chunks[i]
        anchor = chunk[0]
        anchor = (chunk[0] / C * max1[i * length]).to(torch.float32)
        all_tokens += [anchor.unsqueeze(0)]
        for t in range(1, length):
            delta = (chunk[t] / C * max1[i * length + t]).to(torch.float32)
            all_tokens += [(delta + anchor).unsqueeze(0)]


    return torch.cat(all_tokens)

def torch_quant(qA):
    # shape (8, 2048)
    stride = 4
    MAX = int(os.environ["BINS"]) // 2 - 1
    C = MAX
    chunks = torch.chunk(qA, qA.shape[0] // stride)
    all_tokens = []
    all_max = []
    # all_tokens = None
    for i in range(len(chunks)):
        length = len(chunks[i])
        chunk = chunks[i]
        max1 = torch.amax(torch.abs(chunk), dim=-1, keepdim=True)
        xq = torch.round(chunk * (C / max1)).to(torch.int8)
        # if all_tokens is None:
        #     all_tokens = xq[0].unsqueeze(0)
        all_tokens += [xq[0].unsqueeze(0)]
        chunk = chunk - chunk[0] # this is the delta tokens
        max2 = torch.amax(torch.abs(chunk), dim=-1, keepdim=True)
        xq = torch.round(chunk * (C / max2)).to(torch.int8)
        # all_tokens += [xq[1:]]
        # all_tokens = torch.cat((all_tokens, xq[1:]), dim=0)
        for t in range(1, length):
            all_tokens += [xq[t].unsqueeze(0)]
        max1[1:] = max2[1:]
        all_max += [max1]
    return torch.cat(all_tokens), torch.cat(all_max)

class CacheGenEncoder:
    def __init__(self, **kwargs) -> None:
        """
        Fields:
        - fp_kv: should be a tensor of shape (num_layers, num_tokens, num_channels)
        - fp_v: should be a tensor of shape (num_layers, num_tokens, num_channels)
        """
        self.fp_k = kwargs["fp_k"]
        self.fp_v = kwargs["fp_v"]

        self.quantized_key = {}
        self.max_tensors_key = {}
        self.quantized_value = {}
        self.max_tensors_value = {}
        self.config = kwargs["config"]

    def quantize(self):
        """ Quantize the key and value tensors
        (self.fp_k and self.fp_v)
        """
        for layer in range(len(self.fp_k)):

            if layer < self.config["key_first_layers"]:
                os.environ['BINS'] = self.config["key_first_bins"]

            elif layer < self.config["key_second_layers"]:
                os.environ['BINS'] = self.config["key_second_bins"]
            else:
                os.environ['BINS'] = self.config["key_third_bins"]
            tmp = torch_quant(self.fp_k[layer].float())
            self.quantized_key[layer] = tmp[0] + int(os.environ['BINS']) // 2 - 1
            self.max_tensors_key[layer] = tmp[1]
        for layer in range(len(self.fp_v)):
            if layer < self.config["value_first_layers"]:
                os.environ['BINS'] = self.config["value_first_bins"]
            else:
                os.environ['BINS'] = self.config["value_second_bins"]
            tmp = torch_quant(self.fp_v[layer].float())
            self.quantized_value[layer] = tmp[0]+ int(os.environ['BINS']) // 2 - 1
            self.max_tensors_value[layer] = tmp[1]

    def compute_cdf(self,  is_key):
        """
        Compute the CDF based on the quantized tensors
        Field:
        - start_layer: the start layer to compute the CDF
        - end_layer: the end layer to compute the CDF
        """
        # TODO: Add start_index here
        channels = self.fp_k[0].shape[-1]
        tokens = self.fp_k[0].shape[0]

        final_cdf = torch.zeros(len(self.fp_k), channels, 33)

        for i in range(len(self.fp_k)):
            for j in range(channels):

                if is_key:
                    tmp_input = self.quantized_key[i][:, j]
                else:
                    tmp_input = self.quantized_value[i][:, j]
                # if end_layer == 40:
                #     breakpoint()
                symbs_orig, unique_tensor = torch.tensor(tmp_input).unique(return_counts=True)
                output_cdf = torch.zeros(32 )
                output_cdf[symbs_orig.long()] = unique_tensor.float()
                output = output_cdf / output_cdf.sum()
                output = torch.cumsum(output_cdf ,dim=-1) / max(torch.cumsum(output_cdf , dim=-1))
                output =  torch.cat((torch.tensor([0.0]), output))
                final_cdf[i, j] = output

        return final_cdf

def transform_tuple_to_tensors(kv):
    """ Takes in a tuple of #layers tuples, each of the tuple
    is (key_tensor, value_tensor) and returns two tensors of shape
    (# layers, # tokens, # heads * head_dim)
    """
    head_num = kv[0][0].shape[1]
    head_dim = kv[0][0].shape[3]
    tokens_num = kv[0][0].shape[2]
    k_tensor = torch.zeros((len(kv), tokens_num, head_num * head_dim))
    v_tensor = torch.zeros((len(kv), tokens_num, head_num * head_dim))
    for i in range(len(kv)):
        k_tensor[i] = kv[i][0].permute(0, 2, 1, 3).reshape(tokens_num, head_num * head_dim)
        v_tensor[i] = kv[i][1].permute(0, 2, 1, 3).reshape(tokens_num, head_num * head_dim)
    return k_tensor, v_tensor

def _renorm_cast_cdf_(cdf, precision):
    """ The cdf normalization function in torchac
    """
    Lp = cdf.shape[-1]
    finals = 1  # NHW1
    # RENORMALIZATION_FACTOR in cuda
    f = torch.tensor(2, dtype=torch.float32, device=cdf.device).pow_(precision)
    cdf = cdf.mul((f - (Lp - 1)) / finals)  # TODO
    cdf = cdf.round()
    cdf = cdf.to(dtype=torch.int16, non_blocking=True)
    r = torch.arange(Lp, dtype=torch.int16, device=cdf.device)
    cdf.add_(r)
    return cdf

def concat_max(max1):
    """
    Given a dict of max tensors, concatenate them into a single tensor
    """
    maxes = []
    for i in range(len(max1)):
        maxes.append(max1[i].unsqueeze(0))
    return torch.cat(maxes, dim=0)

def concat_dict(dict1, start, end):
    concat_tensor = None
    for i in range(start, end):
        if concat_tensor is None:
            concat_tensor = dict1[i].unsqueeze(0)
        else:
            concat_tensor = torch.cat((concat_tensor, \
                dict1[i].unsqueeze(0)), dim=0)
    return concat_tensor

def encode_function(orig_kv, config, CHUNK_SIZE, output_path, c):
    """
    Given the path to the original key value cache, encode the KV cache
    Fields:
    - path_to_original_kv: the path to the original key value cache (Tuples)
    - config: the path to the quantization config
    - CHUNK_SIZE: the chunk size to encode, NEEDS to be multiples of 20!!!
    - output_path: the path to the output file
    - c: The chunk index
    """
    output_dict = {}
    X = orig_kv
    fp_k, fp_v = transform_tuple_to_tensors(X)
    l = fp_k.shape[0]
    encoder = CacheGenEncoder(fp_k=fp_k, fp_v=fp_v, config=config)
    encoder.quantize()

    cdf_k = encoder.compute_cdf(is_key=True)
    encode_input_key = concat_dict(encoder.quantized_key, 0, config["key_first_layers"])
    encode_input_key = torch.cat((encode_input_key,
                                 concat_dict(encoder.quantized_key, config["key_first_layers"], config["key_second_layers"]) ),
                                 dim=0)
    encode_input_key = torch.cat((encode_input_key,
                                    concat_dict(encoder.quantized_key, config["key_second_layers"], l) ),
                                 dim=0)

    cdf_v = encoder.compute_cdf(is_key=False)
    encode_input_value = concat_dict(encoder.quantized_value, 0, config["value_first_layers"])
    encode_input_value = torch.cat((encode_input_value, concat_dict(encoder.quantized_value, config["value_first_layers"], l) ), dim=0)
    cdf = torch.cat((cdf_k, cdf_v), dim=0)
    encode_input = torch.cat((encode_input_key, encode_input_value), dim=0)
    bitstreams = b""
    maxsize = 1024 * 160 * CHUNK_SIZE
    encode_function.BUFFER = np.zeros(maxsize, dtype=np.uint8)
    buffer = encode_function.BUFFER
    current_index = 0
    start_indices = []
    return cdf, encoder

def transform(X):
    return np.concatenate(([X[0]], np.diff(X)))

def entropy(pk):
    return -np.sum(pk * np.log2(pk+1e-10))

def est_single_size(cdf):
    tot = 0
    input_distr = cdf
    pk = np.array(transform(np.array(input_distr) ))
    base = pk.shape[-1]
    H = entropy(pk)
    return H

key_first_layers = 10 # @param {type:"integer"}
key_second_layers = 20 # @param {type:"integer"}
key_third_layers = 32 # @param {type:"integer"}
value_first_layers = 2 # @param {type:"integer"}
key_first_bins = 32 # @param {type:"integer"}
key_second_bins = 16 # @param {type:"integer"}
key_third_bins = 16 # @param {type:"integer"}
value_first_bins = 32 # @param {type:"integer"}
value_second_bins = 16 # @param {type:"integer"}

#@markdown </br>

#@markdown ### Configure Parameters for the Model

#@markdown The following settings by default belong to the `mistralai/Mistral-7B-v0.1` model.

#@markdown Do not change these settings unless the model is changed as well.

nlayers = 32 # @param {type:"integer"}
nheads = 8 # @param {type:"integer"}
heads_dim = 128 # @param {type:"integer"}

# Settings complete!
# Now we actually create the configurations

CACHE_STORE = None # Used to store KV Cache

quantization_config = {
    "key_first_layers": key_first_layers,
    "key_second_layers": key_second_layers,
    "key_third_layers": key_third_layers,
    "value_first_layers": value_first_layers,
    "key_first_bins": str(key_first_bins),
    "key_second_bins": str(key_second_bins),
    "key_third_bins": str(key_third_bins),
    "value_first_bins": str(value_first_bins),
    "value_second_bins": str(value_second_bins)
}

model_config = {
    "nlayers": nlayers,
    "nheads": nheads,
    "heads_dim": heads_dim
}

#model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2").cuda()
#tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
model = AutoModelForCausalLM.from_pretrained("lmsys/longchat-7b-16k").cuda()
tokenizer = AutoTokenizer.from_pretrained("lmsys/longchat-7b-16k")

input_text = "The weather is good today! I am going to play with my friends."
input_ids = tokenizer(input_text, return_tensors="pt").input_ids.cuda()
output = model.generate(input_ids, do_sample=False, max_new_tokens=1, return_dict_in_generate=True)
print("Output: ")
cdf, encoder = encode_function(output['past_key_values'], quantization_config, 7, "encoded.pkl", 0)
average = []
for layer in range(len(cdf)):
  for channel in range(len(cdf[0])):
    average += [est_single_size(cdf[layer, channel])]
print("Average Entropy: ", np.mean(average))
CACHE_STORE = output['past_key_values']
quantized_k = concat_dict(encoder.quantized_key, 0, model_config['nlayers'])
quantized_v = concat_dict(encoder.quantized_value, 0, model_config['nlayers'])
key = torch.zeros(quantized_k.shape)
max_tensors_k = concat_max(encoder.max_tensors_key)
max_tensors_v = concat_max(encoder.max_tensors_value)
value = torch.zeros(quantized_v.shape)
for l in range(len(quantized_k)):
  if l < quantization_config['key_first_layers' ]:
    os.environ['BINS'] = quantization_config['key_first_bins']
  elif l < quantization_config['key_second_layers']:
    os.environ['BINS'] = quantization_config['key_second_bins']
  else:
    os.environ['BINS'] = quantization_config['key_third_bins']

  key[l] = dequant(quantized_k[l] - int(os.environ['BINS']) // 2 + 1, max_tensors_k[l])
for l in range(value.shape[0]):
    if l < quantization_config["value_first_layers"]:
        os.environ['BINS'] = quantization_config["value_first_bins"]
    else:
        os.environ['BINS'] = quantization_config["value_second_bins"]
    value[l] = dequant(quantized_v[l] - int(os.environ['BINS']) // 2 + 1, max_tensors_v[l])

new_kv = []

# reshape decompressed kv cache to HF's required KV cache
for i in range(32):
  k_shape = CACHE_STORE[0][0].shape
  v_shape = CACHE_STORE[0][1].shape
  k = key[i].reshape((k_shape[0], k_shape[2], k_shape[1], k_shape[3])).permute((0, 2, 1, 3))
  v = value[i].reshape((v_shape[0], v_shape[2], v_shape[1], v_shape[3])).permute((0, 2, 1, 3))
  new_kv += [(k[:, :, :-1].cuda(), v[:, :, :-1].cuda())]
new_kv = tuple(new_kv)

output = model.generate(input_ids, max_new_tokens=20,  past_key_values=new_kv)
print(tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True))